from functools import reduce
from pyspark.sql import DataFrame

df = spark.read.csv("caracteristiques*.csv", header = "true", inferSchema= True, encoding = "ISO-8859-1")
df2009 = spark.read.csv("2009_caracteristiques.csv", header = "true", inferSchema= True, encoding = "ISO-8859-1", sep = '\t')
dfs = [df, df2009]
df_complete = reduce(DataFrame.unionAll, dfs)
df_complete.show()

com = spark.read.csv("code-postal-code-insee-2015.csv", header = "true", inferSchema= True, encoding = "ISO-8859-1", sep = ";")

df1 = df_complete.select("an", "com", "dep")
#suppretion dernier 0 
df1 = df1.withColumn('dep1', when((expr("substring(dep, 3)") == 0) | (expr("substring(dep, 2)") == 0), expr("substring(dep, 1, length(dep)-1)")).otherwise(col("dep")))
# concat de 2 colonne : 
df2=df1.select("*", concat(col("dep1"),lit(""),col("com")).alias("code_isee"))

com1 = com.select("INSEE_COM", "Code_postal", "NOM_COM")

dfcom = df2.join(com1, df2.code_isee == com1.INSEE_COM, 'inner')
final = dfcom.groupBy("an", "Code_postal", "NOM_COM").count().sort("count",ascending=False)


#suppretion dernier 0 
from pyspark.sql.functions import substring, length, col, expr
df1 = df1.withColumn('dep1', expr("substring(dep, 1, length(dep)-1)"))

df1 = df1.withColumn('dep1', when((expr("substring(dep, 3)") == 0) | (expr("substring(dep, 2)") == 0), expr("substring(dep, 1, length(dep)-1)")).otherwise(col("dep")))

# concat de 2 colonne : 
df2=df1.select("*", concat(col("dep1"),lit(""),col("com")).alias("code_isee"))
